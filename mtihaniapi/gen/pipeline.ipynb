{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e21ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from gen.curriculum import get_cbc_grouped_questions, get_rubrics_by_sub_strand\n",
    "from gen.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b3dd6",
   "metadata": {},
   "source": [
    "## Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b14605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Question breakdown written to output/question_breakdown.json. Total: 1\n"
     ]
    }
   ],
   "source": [
    "grouped_questions = get_cbc_grouped_questions(\n",
    "    strand_ids=[1], \n",
    "    question_count=2,\n",
    "    bloom_skill_count=1,\n",
    "    is_debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a45f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Laboratory Apparatus and Instruments =========\n",
      "üìù Input token count (gpt-4o): 708\n",
      "üì¶ Raw LLM output:\n",
      " content='[\\n  {\\n    \"question\": \"In the school laboratory, Amina is using a Bunsen burner to heat a beaker of water. Describe how she should adjust the air hole of the Bunsen burner to achieve a blue flame and explain why this type of flame is preferred for heating.\",\\n    \"expected_answer\": \"Amina should open the air hole of the Bunsen burner to allow more air to mix with the gas, producing a blue flame. This type of flame is preferred because it is hotter and provides more efficient heating.\"\\n  },\\n  {\\n    \"question\": \"During a science experiment, Brian notices that the readings on the thermometer are fluctuating. Analyze the possible reasons for this fluctuation and suggest how he can ensure more stable readings.\",\\n    \"expected_answer\": \"The fluctuation could be due to the thermometer not being properly immersed in the liquid or external temperature changes. To ensure stable readings, Brian should ensure the thermometer is fully immersed and wait for the liquid to reach thermal equilibrium before taking a reading.\"\\n  }\\n]' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 213, 'prompt_tokens': 715, 'total_tokens': 928, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BWiU6LjrIEGw9vc6EBEC5VvHKb3An', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--fda001e1-4d9b-469c-bc96-ac709f02a9ae-0' usage_metadata={'input_tokens': 715, 'output_tokens': 213, 'total_tokens': 928, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "üì§ Output token count (gpt-4o): 212\n",
      "üî¢ Total token usage: 920\n",
      "\n",
      "‚úÖ Question list written to output/question_list.json. Total: 2\n"
     ]
    }
   ],
   "source": [
    "all_question_list = generate_llm_question_list(\n",
    "    grouped_question_data=grouped_questions,\n",
    "    is_debug=True,\n",
    ")\n",
    "\n",
    "# If there was a generation error\n",
    "if not isinstance(all_question_list, list):\n",
    "    print(all_question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54212919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Question list to output/question_list.json. Total: 2\n"
     ]
    }
   ],
   "source": [
    "exam_questions = get_db_question_objects(\n",
    "    all_question_list=all_question_list,\n",
    "    is_debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd1ee3",
   "metadata": {},
   "source": [
    "## Simulate student answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f955561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'question': 'In the school laboratory, Amina is using a Bunsen burner to heat a beaker of water. Describe how she should adjust the air hole of the Bunsen burner to achieve a blue flame and explain why this type of flame is preferred for heating.', 'expected_answer': 'Amina should open the air hole of the Bunsen burner to allow more air to mix with the gas, producing a blue flame. This type of flame is preferred because it is hotter and provides more efficient heating.'}, {'id': 2, 'question': 'During a science experiment, Brian notices that the readings on the thermometer are fluctuating. Analyze the possible reasons for this fluctuation and suggest how he can ensure more stable readings.', 'expected_answer': 'The fluctuation could be due to the thermometer not being properly immersed in the liquid or external temperature changes. To ensure stable readings, Brian should ensure the thermometer is fully immersed and wait for the liquid to reach thermal equilibrium before taking a reading.'}]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_single_qa_list(data):\n",
    "    result = []\n",
    "    for index, item in enumerate(data):\n",
    "        questions = item[\"questions\"]\n",
    "        expected_answers = item[\"expected_answers\"]\n",
    "\n",
    "        if isinstance(questions, str):\n",
    "            questions = [questions]\n",
    "        if isinstance(expected_answers, str):\n",
    "            expected_answers = [expected_answers]\n",
    "\n",
    "        if questions and expected_answers:\n",
    "            result.append({\n",
    "                \"id\": index + 1,\n",
    "                \"question\": questions[0],\n",
    "                \"expected_answer\": expected_answers[0]\n",
    "            })\n",
    "\n",
    "    return result\n",
    "\n",
    "exam_output = convert_to_single_qa_list(exam_questions)\n",
    "print(exam_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfc4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_students = [{'id': 1, 'avg_score': 12}, {'id': 2, 'avg_score': 95}, {\n",
    "    'id': 3, 'avg_score': 54}, {'id': 4, 'avg_score': 71},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42fdf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Input token count (gpt-4o): 715\n",
      "üì¶ Raw LLM output:\n",
      " content='```json\\n[\\n  {\\n    \"id\": 1,\\n    \"answers\": [\\n      {\\n        \"question_id\": \"1\",\\n        \"answer\": \"Amina should open the air hole a bit to get a blue flame. It\\'s better for heating.\"\\n      },\\n      {\\n        \"question_id\": \"2\",\\n        \"answer\": \"The thermometer might be moving or not in the liquid right. He should keep it still.\"\\n      }\\n    ]\\n  },\\n  {\\n    \"id\": 2,\\n    \"answers\": [\\n      {\\n        \"question_id\": \"1\",\\n        \"answer\": \"Amina should open the air hole fully to mix more air with the gas, making a blue flame. This flame is hotter and heats things faster.\"\\n      },\\n      {\\n        \"question_id\": \"2\",\\n        \"answer\": \"The readings might change because the thermometer is not fully in the liquid or the room temperature is changing. Brian should make sure the thermometer is fully immersed and wait until the temperature stops changing before reading.\"\\n      }\\n    ]\\n  },\\n  {\\n    \"id\": 3,\\n    \"answers\": [\\n      {\\n        \"question_id\": \"1\",\\n        \"answer\": \"Amina should open the air hole to get a blue flame. It\\'s better for heating because it\\'s hotter.\"\\n      },\\n      {\\n        \"question_id\": \"2\",\\n        \"answer\": \"The thermometer might not be in the liquid properly or the room is changing temperature. Brian should put it in the liquid fully and wait a bit.\"\\n      }\\n    ]\\n  },\\n  {\\n    \"id\": 4,\\n    \"answers\": [\\n      {\\n        \"question_id\": \"1\",\\n        \"answer\": \"Amina should open the air hole to let more air in, making a blue flame. This flame is hotter and good for heating.\"\\n      },\\n      {\\n        \"question_id\": \"2\",\\n        \"answer\": \"The thermometer readings might change because it\\'s not fully in the liquid or the room temperature is changing. Brian should make sure it\\'s fully in the liquid and wait for it to settle.\"\\n      }\\n    ]\\n  }\\n]\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 434, 'prompt_tokens': 686, 'total_tokens': 1120, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BWiUE8sKdSuAnorqK4zKE7mdG5VmV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--74008304-aa59-4457-854f-b9ef1a323f74-0' usage_metadata={'input_tokens': 686, 'output_tokens': 434, 'total_tokens': 1120, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "üì§ Output token count (gpt-4o): 429\n",
      "üî¢ Total token usage: 1144\n",
      "\n",
      "‚úÖ Mocked Answers list written to output/answers_list.json\n"
     ]
    }
   ],
   "source": [
    "parsed_answers = generate_llm_exam_answers_list(\n",
    "    llm=OPENAI_LLM_4O,\n",
    "    exam_data=exam_output,\n",
    "    student_data=sample_students,\n",
    "    is_debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fe8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== DB MOCK ANSWERS\n",
    "# # ====================\n",
    "# exam = []\n",
    "# EXAM_FILE = \"data/exam.json\"\n",
    "\n",
    "# with open(EXAM_FILE, \"r\") as f:\n",
    "#     exam = json.load(f)\n",
    "\n",
    "# print(exam)\n",
    "\n",
    "\n",
    "# students = []\n",
    "# STUDENT_FILE = \"data/classroom.json\"\n",
    "\n",
    "# with open(STUDENT_FILE, \"r\") as f:\n",
    "#     students = json.load(f)\n",
    "\n",
    "# print(students)\n",
    "\n",
    "\n",
    "# parsed_answers = generate_llm_exam_answers_list(\n",
    "#     llm=OPENAI_LLM_4O,\n",
    "#     exam_data=exam,\n",
    "#     student_data=students,\n",
    "#     is_debug=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd46fe",
   "metadata": {},
   "source": [
    "## Grade answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509a6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_for_question(target_question_id, start_count):\n",
    "    result = []\n",
    "    count = start_count\n",
    "    for entry in parsed_answers:\n",
    "        for answer in entry[\"answers\"]:\n",
    "            if answer[\"question_id\"] == str(target_question_id):\n",
    "                result.append({\n",
    "                    \"answer_id\": count,\n",
    "                    \"answer\": answer[\"answer\"],\n",
    "                })\n",
    "                count += 1\n",
    "    return result, count\n",
    "\n",
    "\n",
    "grouped_answers_data = []\n",
    "count = 1\n",
    "\n",
    "for idx, q in enumerate(exam_questions):\n",
    "    item = {\n",
    "        \"question\": q[\"description\"],\n",
    "        \"expected_answer\": q[\"expected_answer\"],\n",
    "        \"rubrics\": get_rubrics_by_sub_strand(\n",
    "            sub_strand_name=q[\"sub_strand\"],\n",
    "            curriculum_file=\"data/cbc_data.json\",\n",
    "        )\n",
    "    }\n",
    "    student_answers, count = get_answers_for_question(idx + 1, count)\n",
    "    item[\"student_answers\"] = student_answers\n",
    "\n",
    "    grouped_answers_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78940ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mocked Answers list written to output/answers_list.json\n"
     ]
    }
   ],
   "source": [
    "ANSWERS_LIST_OUTPUT_FILE = \"output/answers_list.json\"\n",
    "with open(ANSWERS_LIST_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(grouped_answers_data, f, ensure_ascii=False, indent=4)\n",
    "print(f\"‚úÖ Mocked Answers list written to {ANSWERS_LIST_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44699d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the school laboratory, Amina is using a Bunsen burner to heat a beaker of water. Describe how she should adjust the air hole of the Bunsen burner to achieve a blue flame and explain why this type of flame is preferred for heating. =========\n",
      "üìù Input token count (gpt-4o): 575\n",
      "üì¶ Raw LLM output:\n",
      " content='```json\\n[{\"answer_id\": 1, \"score\": 2}, {\"answer_id\": 2, \"score\": 4}, {\"answer_id\": 3, \"score\": 3}, {\"answer_id\": 4, \"score\": 3}]\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 582, 'total_tokens': 640, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BWiULpLE0VeHcG0HEGBi8I8BD1xNV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9c2a1fa4-4b45-4771-a765-35d31208a4f8-0' usage_metadata={'input_tokens': 582, 'output_tokens': 58, 'total_tokens': 640, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "üì§ Output token count (gpt-4o): 53\n",
      "üî¢ Total token usage: 628\n",
      "\n",
      "During a science experiment, Brian notices that the readings on the thermometer are fluctuating. Analyze the possible reasons for this fluctuation and suggest how he can ensure more stable readings. =========\n",
      "üìù Input token count (gpt-4o): 584\n",
      "üì¶ Raw LLM output:\n",
      " content='```json\\n[\\n    {\"answer_id\": 5, \"score\": 2},\\n    {\"answer_id\": 6, \"score\": 4},\\n    {\"answer_id\": 7, \"score\": 3},\\n    {\"answer_id\": 8, \"score\": 3}\\n]\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 591, 'total_tokens': 654, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BWiUN8lkD2GOu5mr12kcHhsIaFIti', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e4b0ac20-ccd5-43de-b9cc-ba30099812cf-0' usage_metadata={'input_tokens': 591, 'output_tokens': 63, 'total_tokens': 654, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "üì§ Output token count (gpt-4o): 58\n",
      "üî¢ Total token usage: 642\n",
      "\n",
      "‚úÖ Graded Answers list written to output/grades_list.json\n"
     ]
    }
   ],
   "source": [
    "parsed_grades = generate_llm_answer_grades_list(\n",
    "    llm=OPENAI_LLM_4O,\n",
    "    grouped_answers_data=grouped_answers_data,\n",
    "    is_debug=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
