{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acc7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melaniefayne/Desktop/mtihani/mtihani_api/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from gen.curriculum import get_cbc_grouped_questions\n",
    "from gen.utils import *\n",
    "from huggingface_hub import InferenceClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5402213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Question breakdown written to /Users/melaniefayne/Desktop/mtihani/mtihani_api/mtihaniapi/gen/output/question_breakdown.json. Total: 10\n"
     ]
    }
   ],
   "source": [
    "grouped_questions = get_cbc_grouped_questions(\n",
    "    strand_ids=[2,4,1,6,3,8,9],\n",
    "    question_count=10,\n",
    "    bloom_skill_count=2,\n",
    "    is_debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4de0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "LLAMA_3_7_B = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "LLAMA_3_8_B = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "MIXTRAL_8_7_B =\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "TINY_LLAMA = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "LLAMA_3_2_VISION=\"meta-llama/Llama-3.2-11B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b6622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_strand_questions(\n",
    "    sub_strand_data: Dict[str, Any],\n",
    "    llm: str,\n",
    ") -> str:\n",
    "    prompt_template = CREATE_EXAM_LLM_PROMPT\n",
    "    formatted_prompt = prompt_template.format(\n",
    "        strand=sub_strand_data[\"strand\"],\n",
    "        sub_strand=sub_strand_data[\"sub_strand\"],\n",
    "        learning_outcomes=sub_strand_data[\"learning_outcomes\"],\n",
    "        skills_to_assess=sub_strand_data[\"skills_to_assess\"],\n",
    "        skills_to_test=sub_strand_data[\"skills_to_test\"],\n",
    "        question_count=sub_strand_data[\"question_count\"],\n",
    "    )\n",
    "\n",
    "    response = client.text_generation(\n",
    "        model=llm,\n",
    "        prompt=formatted_prompt,\n",
    "        max_new_tokens=10240,\n",
    "        temperature=0.1,\n",
    "        stream=False,  # Set to True for line-by-line tokens\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97f747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_question_list(\n",
    "    grouped_question_data: List[Dict[str, Any]],\n",
    "    llm: Any,\n",
    "    output_file: str = QUESTION_LIST_OUTPUT_FILE,\n",
    ") -> Union[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    all_question_list = []\n",
    "\n",
    "    for group in grouped_question_data:\n",
    "        strand = group[\"strand\"]\n",
    "        sub_strand = group[\"sub_strand\"]\n",
    "        learning_outcomes = \"\\n- \" + \"\\n- \".join(group[\"learning_outcomes\"])\n",
    "        skills_to_assess = \"\\n- \" + \"\\n- \".join(group[\"skills_to_assess\"])\n",
    "\n",
    "        # Step 1: Flatten all skills with their associated breakdown number\n",
    "        numbered_skills = []\n",
    "        for entry in group[\"skills_to_test\"]:\n",
    "            number = entry[\"number\"]\n",
    "            for skill in entry[\"skills_to_test\"]:\n",
    "                numbered_skills.append({\"number\": number, \"skill\": skill})\n",
    "\n",
    "        # Step 2: Build a flat list of just the skills (in order)\n",
    "        skills_only = [entry[\"skill\"] for entry in numbered_skills]\n",
    "\n",
    "        # Step 3: Generate all questions in one LLM call\n",
    "        sub_strand_data = {\n",
    "            \"question_count\": len(skills_only),\n",
    "            \"strand\": strand,\n",
    "            \"sub_strand\": sub_strand,\n",
    "            \"learning_outcomes\": learning_outcomes,\n",
    "            \"skills_to_assess\": skills_to_assess,\n",
    "            \"skills_to_test\": skills_only,\n",
    "        }\n",
    "       \n",
    "        print(f\"\\n{sub_strand} =========\")\n",
    "\n",
    "        parsed_output = generate_sub_strand_questions(\n",
    "            sub_strand_data=sub_strand_data,\n",
    "            llm=llm,\n",
    "        )\n",
    "\n",
    "        print(parsed_output)\n",
    "\n",
    "    #     parsed_output = safe_parse_llm_output(parsed_output)\n",
    "    #     if not parsed_output:\n",
    "    #         print(\"Skipping: could not parse output.\")\n",
    "    #         continue\n",
    "\n",
    "    #     if not parsed_output:\n",
    "    #         print(f\"Skipping sub-strand: {sub_strand} due to un-parse-able output\")\n",
    "    #         continue\n",
    "\n",
    "    #     tagged_responses = []\n",
    "    #     for idx, qa in enumerate(parsed_output):\n",
    "    #         # Prevent index overflow if LLM returns fewer/more questions\n",
    "    #         if idx >= len(numbered_skills):\n",
    "    #             break\n",
    "    #         question_item = {}\n",
    "    #         question_item[\"number\"] = numbered_skills[idx][\"number\"]\n",
    "    #         question_item[\"grade\"] = group[\"grade\"]\n",
    "    #         question_item[\"strand\"] = group[\"strand\"]\n",
    "    #         question_item[\"sub_strand\"] = group[\"sub_strand\"]\n",
    "    #         question_item[\"bloom_skill\"] = numbered_skills[idx][\"skill\"]\n",
    "    #         question_item[\"description\"] = qa[\"question\"]\n",
    "    #         question_item[\"expected_answer\"] = qa[\"expected_answer\"]\n",
    "    #         tagged_responses.append(question_item)\n",
    "    #     all_question_list.extend(tagged_responses)\n",
    "\n",
    "    # all_question_list = sorted(all_question_list, key=itemgetter(\"number\"))\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_question_list, f, ensure_ascii=False, indent=4)\n",
    "    print(\n",
    "        f\"\\n✅ Question list written to {output_file}. Total: {len(all_question_list)}\")\n",
    "\n",
    "    return all_question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea97d20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acids, Bases and Indicators =========\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Non-conversational image-text-to-text task is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m selected_model \u001b[38;5;241m=\u001b[39m LLAMA_3_2_VISION\n\u001b[0;32m----> 2\u001b[0m all_question_list \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_llm_question_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrouped_question_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrouped_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselected_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_question_list)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# If there was a generation error\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mgenerate_llm_question_list\u001b[0;34m(grouped_question_data, llm, output_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m     sub_strand_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(skills_only),\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrand\u001b[39m\u001b[38;5;124m\"\u001b[39m: strand,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskills_to_test\u001b[39m\u001b[38;5;124m\"\u001b[39m: skills_only,\n\u001b[1;32m     32\u001b[0m     }\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msub_strand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     parsed_output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sub_strand_questions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub_strand_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_strand_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(parsed_output)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#     parsed_output = safe_parse_llm_output(parsed_output)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#     if not parsed_output:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#         print(\"Skipping: could not parse output.\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# all_question_list = sorted(all_question_list, key=itemgetter(\"number\"))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mgenerate_sub_strand_questions\u001b[0;34m(sub_strand_data, llm)\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m CREATE_EXAM_LLM_PROMPT\n\u001b[1;32m      6\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      7\u001b[0m     strand\u001b[38;5;241m=\u001b[39msub_strand_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrand\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     sub_strand\u001b[38;5;241m=\u001b[39msub_strand_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_strand\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     question_count\u001b[38;5;241m=\u001b[39msub_strand_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_count\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10240\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True for line-by-line tokens\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/mtihani/mtihani_api/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:2351\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2345\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2346\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pass `stream=False` as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2348\u001b[0m         )\n\u001b[1;32m   2350\u001b[0m provider_helper \u001b[38;5;241m=\u001b[39m get_provider_helper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2351\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/mtihani/mtihani_api/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_providers/_common.py:64\u001b[0m, in \u001b[0;36mTaskProviderHelper.prepare_request\u001b[0;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[1;32m     61\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_api_key(api_key)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m mapped_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_mapped_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_headers(headers, api_key)\n",
      "File \u001b[0;32m~/Desktop/mtihani/mtihani_api/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_providers/hf_inference.py:35\u001b[0m, in \u001b[0;36mHFInferenceTask._prepare_mapped_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no recommended model for HF Inference. Please specify a model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explicitly. Visit https://huggingface.co/tasks for more info.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[0;32m---> 35\u001b[0m \u001b[43m_check_supported_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_id\n",
      "File \u001b[0;32m~/Desktop/mtihani/mtihani_api/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_providers/hf_inference.py:152\u001b[0m, in \u001b[0;36m_check_supported_task\u001b[0;34m(model, task)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_conversational \u001b[38;5;129;01mand\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversational\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# Only conversational allowed if tagged as conversational\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-conversational image-text-to-text task is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     task \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature-extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m pipeline_tag \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature-extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tags\n\u001b[1;32m    158\u001b[0m ):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# feature-extraction and sentence-similarity are interchangeable for HF Inference\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Non-conversational image-text-to-text task is not supported."
     ]
    }
   ],
   "source": [
    "selected_model = LLAMA_3_2_VISION\n",
    "all_question_list = generate_llm_question_list(\n",
    "    grouped_question_data=grouped_questions,\n",
    "    llm=selected_model,\n",
    ")\n",
    "\n",
    "print(all_question_list)\n",
    "\n",
    "# If there was a generation error\n",
    "if not isinstance(all_question_list, list):\n",
    "    print(all_question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_questions = get_db_question_objects(\n",
    "    all_question_list=all_question_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef97448",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = os.path.join(\n",
    "    BASE_DIR, \"output/model_comparisons\", f\"{selected_model}_EXAM.txt\")\n",
    "\n",
    "def export_exam_questions_to_txt(exam_questions, output_file=OUTPUT_FILE):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, q in enumerate(exam_questions, start=1):\n",
    "            f.write(f\"\\nQuestion {idx}\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Grade: {q.get('grade', '')}\\n\")\n",
    "            f.write(f\"Strand: {q.get('strand', '')}\\n\")\n",
    "            f.write(f\"Sub Strand: {q.get('sub_strand', '')}\\n\\n\")\n",
    "            \n",
    "            # Handle single or multiple bloom skills per question\n",
    "            bloom_skills = q.get(\"bloom_skills\", [])\n",
    "            questions = q.get(\"questions\", [])\n",
    "            answers = q.get(\"expected_answers\", [])\n",
    "            \n",
    "            # If your structure is per-skill, per-QA:\n",
    "            for b_idx, (bloom, ques, ans) in enumerate(zip(bloom_skills, questions, answers), start=1):\n",
    "                f.write(f\"Bloom Skill {b_idx}: {bloom}\\n\")\n",
    "                f.write(f\"      Q: {ques}\\n\")\n",
    "                f.write(f\"      A: {ans}\\n\\n\")\n",
    "            \n",
    "            # If your structure is single-skill per question, you can do:\n",
    "            # f.write(f\"Bloom Skill 1: {q.get('bloom_skill', '')}\\n\")\n",
    "            # f.write(f\"      Q: {q.get('description', '')}\\n\")\n",
    "            # f.write(f\"      A: {q.get('expected_answer', '')}\\n\\n\")\n",
    "                \n",
    "        print(f\"✅ Exam questions exported to {output_file}\")\n",
    "\n",
    "# Usage:\n",
    "# exam_questions = [...]  # your processed list of dicts\n",
    "export_exam_questions_to_txt(exam_questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtihani-api-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
